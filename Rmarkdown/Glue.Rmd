---
title: "Glue"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(Hmisc)
library(tidyverse)
```

# Calibration

Calibration is picking parameter sets based on performance evaluation

Apply metrics over multiple outputs (generated by running across many parameters sets) - like we've done in our sensitivity analysis work

Example - a dataset where each column
is a different model run for Sagehen Creek
(using different parameters) - don't worry about the parameters for now

* sagerm.txt



```{r multipel}

source("../R/nse.R")
# original data set - a single streamflow and observed data
sager = read.table("../Data/sager.txt", header=T)
head(sager)

# add date
sager = sager %>% mutate(date = paste(day,month,year, sep="/"))
sager$date = as.Date(sager$date,"%d/%m/%Y")

# multiple results - lets say we've run the model for multiple years, each column
# is streamflow for a different parameter set
msage = read.table("../Data/sagerm.txt", header=T)

# lets say we know the start date from our earlier output
msage$date = sager$date
head(msage)
msage$month = sager$month
msage$year = sager$year
msage$day = sager$day
msage$wy = sager$wy

# and we still have observed data from above


# how can we plot all results - lets plot water year 1970 otherwise its hard to see
msagel = msage %>% pivot_longer(cols=!c(date, month, year, day,wy), names_to="run", values_to="flow")

p1=ggplot(subset(msagel, wy == 1970), aes(as.Date(date), flow, col=run))+geom_line()+theme(legend.position = "none")
p1
# lets add observed streamflow
p1+geom_line(data=subset(sager, wy == 1970), aes(as.Date(date), obs), size=2, col="black", linetype=2)+labs(y="Streamflow", x="Date")


# compute performance measures for all output
res = msage %>% select(!c("date","month","year","day","wy")) %>%
      map_dbl(nse, o=sager$obs )

head(res)

# another example using our low flow statistics
# use apply to compute for all the data
source("../R/compute_lowflowmetrics_all.R")
res = msage %>% select(-date, -month, -day, -year, -wy ) %>% map_df(compute_lowflowmetrics_all, o=sager$obs, month=msage$month, day=msage$day, year=msage$year, wy=msage$wy)
res = as.data.frame(res)

# create a link to the column names of simulation runs so we can later
# link simulation runs to performance measures
res$run = msage %>% select(-date, -month, -day, -year, -wy ) %>% colnames()

# interesting to look at range of metrics - could use this to decide on
# acceptable values
summary(res)


# graph range of performance measures
resl = res %>% pivot_longer(cols=c(-run), names_to="metric", values_to="value")
ggplot(resl, aes(metric, value))+geom_boxplot()+facet_wrap(~metric, scales="free")


# best one

best = res[which.max(res$combined),]
msagel  =  msage %>% pivot_longer(cols=!c(date, month, year, day,wy), names_to="run", values_to="flow")
ggplot(subset(msagel, run == best$run), aes(date, flow)) + geom_line()


# or show with all runs
# label runs for legends
# all just show 1970 so easy to see
msagel = msagel %>% mutate(best = ifelse(run==best$run, TRUE, FALSE))
ggplot(subset(msagel, wy==1970), aes(date, flow, col=best))+geom_line()

```

# Glue - generalized uncertainty analysis

What if we wanted to keep all of the 'good' parameters

* we could just keep them all as equally likely
* we could weight them by performance

Either way we can graph and come up with 'best' prediction accounting for uncertainty

Create a single measure of accuracy - above we used *compute_lowlowmetrics_all* to compute an accuracy measure based on

* relative error in annual minimum flow estimate
* relative error in monthly flow during low flow period
* correlation between observed and modelled annual minimum flow
* correlation between observed and modelled flow during the low flow period

We weighted all 4 the same

# Use the accuracy measure 

We can use the combined accuacy measure to define behavioural (acceptible) parameter set (**res_acc**) - two options

* define a threshold (we will use 30%)
* take top 50 performing parameter sets

(we go with the latter but code could be commented to go with threshold approach)

```{r behavioral}


summary(res$combined)

# 1) selecting behaviorial or acceptable parameters sets

threshold = 0.3
res_acc = subset(res, combined > threshold)
head(res_acc)

# as an alternative  what if you want the top N parameter sets
topN = 50
tmp = res[order(res$combined,decreasing=T),]  
res_acc=tmp[1:topN,]
head(res_acc)
```

# Defining weights (likelihood) for parameter sets

Now define "weights" (likelihood) based on parameter performance for the acceptable or behaviorial parameters

We want the sum of the weights to equal 1

* accuracy measure defined above will define weight
* we normalize by the range of accuracy for the behavioural parameters  
* this **relative accuracy ** becomes the weight
* note we now only work with behavioural parameter sets (in ** res_acc ** versus ** res **)



```{r weighting}

# create a weight for each parameter set based on its relative accuracy - we do this so all weights sum to 1
max_acc=max(res_acc$combined)
min_acc=min(res_acc$combined)
res_acc$w_acc=(res_acc$combined-min_acc)/(max_acc-min_acc)
sum_acc=sum(res_acc$combined)
res_acc$wt_acc=res_acc$combined/sum_acc

# look at values
summary(res_acc$wt_acc)
# check to see that they sum to one
sum(res_acc$wt_acc)

Nacc = nrow(res_acc)
Nacc
```

# Using weights

One way to use weights is to define a maximum likelihood estimate by averaging (weighted by accuracy) streamflow from all behavioural simulations 



```{r mle}

# generate a streamflow as weighted average of all  acceptable parameter sets

# recall that msagel is the flow data for all runs so we 
# can link with weights from res_acc by run id


# subset only acceptible runs
msagel_acc = subset(msagel, run %in% res_acc$run)
# join with weights from res_acc, left_join will repeat weights for each day in streamflow trajectory
msagel_acc = left_join(msagel_acc, res_acc, by="run")
head(msagel_acc)
# finally multiply flow by weight
msagel_acc = msagel_acc %>% mutate(flow_wt = flow*wt_acc)

# now we can average streamflow for each day from all the runs # using the weights
aver_flow = msagel_acc %>% group_by(date) %>% dplyr::summarize(meanstr = sum(flow_wt))

# add some date information or simply add to simQ

ggplot(aver_flow, aes(x=date, y=meanstr))+geom_line(col="red")+labs(y="Streamflow mm/day")

# add some of the other date info and plot a subset
aver_flow$wy = msage$wy
ggplot(subset(aver_flow, wy == 1980), aes(x=date, y=meanstr))+geom_line(col="red")+labs(y="Streamflow mm/day")

```

We could also compute quantiles rather than just mean


```{r plotting}

# first assume that all streamflow estimates are equally probable
aver_flow = msagel_acc %>% group_by(date) %>% dplyr::summarize(
median = quantile(x=flow, prob=0.5), 
flow25 = quantile(x=flow,prob=0.25), flow75=quantile(x=flow, prob=0.75))
# format for plotting
aver_flowl = aver_flow %>% pivot_longer(col=c(-date), values_to="flow", names_to="quantile")
# plot a single year in log scale so we can really see


ggplot(subset(aver_flowl, format(date,"%Y")==1980), aes(date, flow, col=quantile))+geom_line()
# use log to really see low flow differences
ggplot(subset(aver_flowl, format(date,"%Y")==1980), aes(date, flow, col=quantile))+geom_line()+scale_y_continuous(trans="log")

# wtd quantile can use the unnormalized weights - it does the normalization for you
aver_flow = msagel_acc %>% group_by(date) %>% dplyr::summarize(
median = wtd.quantile(x=flow, prob=0.5, weights=w_acc), 
flow25 = wtd.quantile(x=flow,prob=0.25, weights=w_acc), flow75=wtd.quantile(x=flow, prob=0.75, weights=w_acc))
# format for plotting
aver_flowl = aver_flow %>% pivot_longer(col=c(-date), values_to="flow", names_to="quantile")
ggplot(subset(aver_flowl, format(date,"%Y")==1980), aes(date, flow, col=quantile))+geom_line()
# use log to really see low flow differences
ggplot(subset(aver_flowl, format(date,"%Y")==1980), aes(date, flow, col=quantile))+geom_line()+scale_y_continuous(trans="log")


```
